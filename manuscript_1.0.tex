% declare documents and packages
\documentclass[man, 12pt, a4paper]{apa6}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{times}
\usepackage{endnotes}
% this command put all footnotes to endnote at the end of the paper
\let\footnote\endnote

% for word counts
\newcommand{\detailtexcount}{%
  \immediate\write18{texcount -merge -sum -incbib -dir \jobname.tex > \jobname.wcdetail }%
  \verbatiminput{\jobname.wcdetail}%
}
\newcommand{\quickwordcount}{%
  \immediate\write18{texcount -1 -sum -merge \jobname.tex > \jobname-words.sum }%
  \input{\jobname-words.sum} words%
}
\newcommand{\quickcharcount}{%
  \immediate\write18{texcount -1 -sum -merge -char \jobname.tex > \jobname-chars.sum }%
  \input{\jobname-chars.sum} characters (not including spaces)%
}

% Biblatex
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber,uniquename=false]{biblatex}
% make in-text citations clickable
\usepackage[colorlinks=true]{hyperref}
% this removes annoying colors for clickable in-text citation entries 
\usepackage{xcolor}
\hypersetup{  
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\addbibresource{abm_misinformation_lit.bib}
\DeclareLanguageMapping{american}{american-apa}
 
\title{If you see something, then say something to others: A simple contagion of misinformation and a complex contagion of socially-contingent corrections}
\shorttitle{Socially-contingent corrections}

\author{Hyunjin Song}

\affiliation{Department of Communication, University of Vienna, Austria}


\abstract{As the citizens' news consumption is increasingly driven by online sources, the propagation of misinformation and so-called \enquote{fake news} on those platforms become an increasing concern for the public and policy makers. Our goal in this contribution is to offer a more systematic assessment of underlying mechanisms of misinformation spreading and its correction, combining a macro social contextual factor and individuals' cognitive basis of adopting misinformation into a more integrated, dynamic system model perspective. We first review existing evidence concerning individuals' cognitive basis of adopting such misinformation, and social context of which exposure to misinformation and its corrections are received. Next, adopting a well-known class of an epidemic model of virus infection and recovery, we combine this micro and macro dynamics into comprehensive, integrated model of misinformation diffusion on social networks. We do so by focusing on the distinction between simple contagion of misinformation vs. complex contagion of adopting corrective messages. Relying on Agent-based simulations, we further explore various boundary conditions of such dynamics, aiming to uncover how and when such misinformation propagates into the public, as well as what factors facilitate or hinder such diffusion process.}

\authornote{Hyunjin (Jin) Song is currently an assistant professor ("Universit√§tsassistent, post-doc") in the Department of Communication at the University of Vienna, and also a member of the Vienna Computational Communication Science Lab.}

\note{%Word count: \quickwordcount \\
    Draft date: \today \\
      \textbf{Draft in progress. Please do not cite without permission.} \\
      Please direct any questions and inquiries to \href{mailto:hyunjin.song@univie.ac.at}{hyunjin.song@univie.ac.at}}

\begin{document}
\maketitle
  Citizens across the worlds are experiencing major changes in their news environments with the development of digital media. One of the most dramatic changes in the news environment in recent decades involves the role social networking sites (SNS) such as Facebook and Twitter play as a primary source of news outlets. Not only citizens' news consumptions are increasingly driven by such online sources \parencite{shearer2017news}, but it also appears that citizens themselves are actively participating in news dissemination on those platforms by sharing news contents with their peers \parencite[e.g.,][]{weeks2013dissemination, lee2017people}. 

  An effective deliberation among public is regarded as a keystone of thriving democracies, and modern political systems squarely depend  on  informed  decisions of citizens in that regard \parencite{carpini1996, stromback2005search}. Yet, a propagation of rumors, misinformation, and so-called \enquote{Fake news} on those platforms becomes an increasing concern for the public and policy makers alike \parencite{allcott2017social}. There are abundant, yet still fragmentary, evidence of viral spreads of unsubstantiated, often factually dubious (mis)information that potentially affecting millions of citizens across the globe -- as evidenced in recent 2016 U.S. presidential election \parencite{guess2018selective, giglietto2016fakes, allcott2017social} and in Brexit votes \parencite{nyt_2017}. While a wide circulation of factually dubious information is not entirely new to politics, a growing trend of digitally disseminated rumors and misinformations -- often termed as a \enquote{Fake news} phenomenon -- is increasingly recognized as a serious threat to liberal democratic societies \parencite{allcott2017social}. Either based on unsubstantiated rumors or based on factually wrong beliefs, many of the misinformed behave differently than those who are accurately informed \parencite{kuklinski2000misinformation,Garrett_Weeks_2013}. They often disagree about basic facts about many political and public issues \parencite[e.g.,][]{nisbet2015partisan}, and continue to believe and rely on such false information when evaluating political matters \parencite{nyhan2010corrections,thorson_2016}. 

  Along with these trends, there has been an growing interest among scholars on how people process and maintain factually false (or at least factually dubious) information from the perspectives of an individual's cognitive and affective mechanisms \parencite{Lewandowsky_2012PSPI, kuklinski2000misinformation, garrett2016driving, weeks2015emotions}. These studies have generated a valuable insights of how individuals maintain false beliefs, and how corrections to such false beliefs are received and processed under various scenarios \parencite{Lewandowsky_2012PSPI, thorson_2016}. However, despite growing interest and continued research effort to better understand the nature and its exact mechanism, what we know about the spread of misinformation and fake news on online social networks is largely based on limited evidence due to its complex nature of the problem.

  Against this backdrop, our goal in this contribution is to offer a more systematic assessment of underlying mechanisms of misinformation spreading and its correction, focusing on one's \emph{social contexts} in which such (mis)information and corrective messages are received and processed. We argue that while \emph{exposure} to (mis)information is likely to follow a simple contagion process, \emph{changes} in one's beliefs regarding such (mis)information -- which ultimately \emph{the} goal of corrective messages -- is likely to be, in \citeauthor{centola2007complex}'s (\citeyear{centola2007complex}) term, a \enquote{complex contagion} where such changes require multiple sources of affirmation and reinforcement compared to simple contagion process. As a result, the effects of fact-checking and corrective messages are likely to be highly \emph{socially} contingent, yet the vast majority of prior studies have not considered this possibility \parencite{margolin2017, bode2017see}.    

  In what follows, we first review existing evidence regarding political misperceptions and the effect of fact-checking (i.e., correction) messages. We advance our dynamic system perspective by combining an individual-level cognitive and affective basis of adopting such misinformation with a social context of which an exposure to misinformation and corrections are received. Based on a well-known class of an epidemic model of virus infection and recovery, we propose an integrated model of misinformation diffusion and socially-contingent corrections on social networks, with a special focus on the differences between simple contagion of misinformation and complex contagion of corrections. Relying on Agent-based simulations, we further explore boundary conditions of such dynamics, aiming to uncover how and when such misinformation propagates into the public, as well as what factors facilitate or hinder such diffusion process.

\section{A Psychology of Fake News, Misperceptions, and Corrections}

     Following \citeauthor{allcott2017social}'s (\citeyear{allcott2017social}) definition, we define \emph{Fake news} as \enquote{distorted signals uncorrelated with the truth} (p. 212). This encompasses several related concepts, such as misinformation, rumors, and disinformation. Literature on this topic generally maintain loosely defined, but at the same time highly interrelated, definitions of those related terms. For instance, (political) rumors are often defined as \enquote{unsubstantiated claims about candidates and issues that are often false} \parencite[p. 401]{weeks2014electoral}. Similarly, misinformation (or misperceptions) are defined as factual information (or beliefs) \enquote{that are false or contradict the best available evidence in the public domain} \parencite[p. 128]{flynn2017nature}. In relation to this, \emph{dis}information campaigns often denote organized, strategic efforts that trying to sway public opinion using rumors and misinformation \parencite{Garrett2017distraction, Lewandowsky_2012PSPI}. Understood in this way, fake news often exclude unintentional reporting mistakes, parodies and satires, or unverifiable conspiracy theories \parencite{allcott2017social}. While term \emph{fake news} often than not additionally entail specific pseudo-journalistic styles that mimic legitimate news sources to intentionally deceiving audiences \parencite{jana_sophie_fn}, we use term \enquote{fake news} somewhat loosely, denoting any type of misinformation -- information that is not supported by best-available evidence -- that is deliberatively circulated among publics.\footnote{ Often, the term \emph{fake news} is used as derogatory, rhetorical label to attack political opponents. While such use of the term as a \emph{label} is an important conceptual dimension to consider, this aspects of \emph{fake news} is beyond the scope of this manuscript. See \citeauthor{jana_sophie_fn} (\citeyear{jana_sophie_fn}) instead for a detailed conceptualization involving this distinction.}
     
      Literature on misinformation and its persistence often converges to the observation that publics' exposure to and acceptance of misinformation are largely driven by one's motivated consistency needs. That is, people are more often than not likely to be disproportionately drawn by information that conforms to their partisan priors \parencite{guess2018selective, weeks2014electoral}, and also more likely to accept and endorse such messages \parencite{kunda1990,nyhan2010corrections}. A mounting evidence -- largely based on \citeauthor{kunda1990}'s (\citeyear{kunda1990}) or on \citeauthor{taber2006}'s (\citeyear{taber2006}) motivated reasoning framework -- suggests that citizens tend to evaluate attitudinally congruent information as more convincing and valid \emph{regardless of its truth-value}, while inconsistent information is likely to be perceived as weak and therefore likely to be rejected \parencite{taber2006,weeks2015emotions}. Therefore, it is perhaps not surprising to find that most of the prior studies based on motivated reasoning framework document that fact-checking messages (sometimes denoted as \enquote{corrective} or \enquote{debunking} messages in the literature) have only limited effects due to inherent tendency of humans to directionally process (politically) relevant information \parencite{thorson_2016, taber2006,flynn2017nature}. Even worse, corrective messages may backfire, may induce higher level of endorsements of false beliefs than actually lower them \parencites[e.g.,][]{nyhan2010corrections}[but see][]{Wood2018}.
      
      Another line of studies based on a dual process theory of human cognitive processing and memory suggests that attitudinally-congruent misinformation creates automatic and strong affective responses (i.e., automatically and effortlessly activated), whereas attitudinally incongruent correction messages rarely produce such responses. Due to such asymmetrical nature, people have to rely on more deliberative, strategic processes (which require significant cognitive resources) to recall attitudinally inconsistent correction messages and incorporate them into relevant judgments \parencite{thorson_2016, Lewandowsky_2012PSPI, SwireEcker2018}. Also, since misinformations tend to form a coherent mental model of a relevant event based on their partisan schema and stereotypes \parencite[e.g.,][]{garrett2013undermining}, people tend to fill any gaps caused by corrections (that invalidate some parts of their existing mental model) with a flawed but attitudinally congruent misinformation that is still readily accessible in their memory \parencite{Lewandowsky_2012PSPI, SwireEcker2018}. Studies also find that this effect is much more likely when correction messages do not update the initial mental model that justifies misinformation \parencite{Chan_debunking_meta_2017, SwireEcker2018}, when the perceived veracity of initial misinformation is high (due to fluency bias in one's cognitive processing) \parencite{SwireEcker2018, Lewandowsky_2012PSPI}, or when individuals can generate (counter-arguing) reasons in support for initial misperceptions \parencite{garrett2013undermining, Chan_debunking_meta_2017}. Most importantly, due to aforementioned limitations of strategic memory processes (which require effortful processing), people may still rely on negated misinformation in subsequent reasoning \emph{even when they remember such information is factually incorrect} \parencite{Lewandowsky_2012PSPI}. Therefore, even in the face of seemingly effective corrections, the effect of misperceptions lingers and continue to exert influence \parencite[e.g.,][]{thorson_2016}.  
      
        Under certain situations, it appears that citizens \emph{indeed} can adhere factual information based on correction messages despite of their perpetual partisan bias \parencite[e.g.,][]{Wood2018, Garrett_Weeks_2013, weeks2015emotions}. Yet as \citeauthor{margolin2017} (\citeyear{margolin2017}) note, it appears that such effects often require special \emph{social context}. This observation is indeed much warranted, as most of the previous studies concerning misinformation and the effect of fact-checking messages are conducted in an experiment context with a single-shot, \emph{asocial} correction message from media professionals and fact-checking organizations \parencite[e.g.,][]{nyhan2010corrections,garrett2013undermining,weeks2015emotions}. Much of the literature on partisan selective exposure and political discussion networks already point that social contexts of which an individual is exposed to counter-attitudinal messages may have a powerful consequence on how such messages are interpreted and processed \parencite{messing2014selective, levitan2014seeking, levitan2008resistance}. There is also a suggestive evidence that fact-checking and corrective messages from one's peers in their social networks -- what we would call a \enquote{social correction} -- are more likely to, if not equally, be effective in reducing misperceptions \parencite[e.g.,][]{margolin2017, bode2017see}. In what follow, we review several theoretical accounts of such \emph{socially-based} correction messages on misinformation and fake news.   
      
\section{A \emph{Social} Context of Misperceptions and Corrections: Simple vs. Complex Contagion}

  People's perceptions and behaviors are likely to be interdependent upon their social contacts \parencite{lazer2010coevolution, centola2007complex}, and therefore it is not surprising that such perceptions and behaviors may spread through social networks \parencite[e.g.,][]{christakis2007spread, bond_61million}. Indeed, a non-negligible number of prior accounts concerning the misperception and fake news in social networks connect this idea to possible mechanisms of \emph{spreading} of misinformation \parencite[e.g.,][]{del2016spreading, lazer2017combating, Bessi_2015}.  
    
    



\printbibliography
\newpage
\begingroup
\parindent 0pt
\parskip 2ex
\def\enotesize{\normalsize}
\theendnotes
\endgroup
\end{document}
